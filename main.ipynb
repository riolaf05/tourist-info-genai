{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PyPaperBot import __main__ as pypaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"pdf\"\n",
    "FOLDER_PATH=\"./documents\"\n",
    "MODEL_NAME=\"Llama3-8b-8192\"\n",
    "topic=\"quantum computing\"\n",
    "year=2023\n",
    "pagine_scholar=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['powershell', 'python', '-m', 'PyPaperBot', '--query=quantum+computing', '--scholar-pages=1', '--min-year=2023', '--dwn-dir=./documents', '--restrict=1']\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "print([\"powershell\", \"python\", \"-m\", \"PyPaperBot\", \"--query={}\".format(topic.replace(\" \", \"+\")), \"--scholar-pages={}\".format(pagine_scholar), \"--min-year={}\".format(year), \"--dwn-dir={}\".format(FOLDER_PATH), \"--restrict=1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyPaperBot is a Python tool for downloading scientific papers using Google Scholar, Crossref and SciHub.\n",
      "If you like this project, you can give me a cup of coffee at --> https://www.paypal.com/paypalme/ferru97 <-- :)\n",
      "\n",
      "Scholar results best applied along with --scholar-pages=1\n",
      "Query: quantum+computing\n",
      "\n",
      "Google Scholar page 1 : 1 papers found\n",
      "Searching paper 1 of 1 on Crossref...\n",
      "Papers found on Crossref: 1/1\n",
      "\n",
      "\n",
      "Using https://sci-hub.ee as Sci-Hub instance\n",
      "Download 1 of 1 -> A survey of important issues in quantum computing and communications\n",
      "\n",
      "Work completed!\n",
      "If you like this project, you can offer me a cup of coffee at --> https://www.paypal.com/paypalme/ferru97 <-- :)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m PyPaperBot --query=quantum+computing --scholar-pages=1 --scholar-results=1 --min-year=2023 --dwn-dir=./documents --restrict=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://github.com/ferru97/PyPaperBot/tree/master/PyPaperBot\n",
    "\n",
    "# pypaper.start(\n",
    "#     query=\"Quantum Computing\", \n",
    "#     scholar_pages=[1], \n",
    "#     min_date=2018, \n",
    "#     dwn_dir=\"./documents\", \n",
    "#     # SciHub_URL=\"https://sci-hub.do\", \n",
    "#     proxy=[], \n",
    "#     scholar_results=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from utils import database_managers, embedding, text_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print your results with Markup language\n",
    "def print_result(result):\n",
    "  output_text = f\"\"\"\n",
    "  ### Answer: \n",
    "  {result['answer']}\n",
    "  ### Sources: \n",
    "  {result['sources']}\n",
    "  ### All relevant sources:\n",
    "  {' '.join(list(set([doc.metadata['source'] for doc in result['source_documents']])))}\n",
    "  \"\"\"\n",
    "  return(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=text_processing.TextSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 2436.28it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding = embedding.EmbeddingFunction('fast-bgeEmbedding').embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection pdf already exists!\n"
     ]
    }
   ],
   "source": [
    "vectore_store=qdrantClient = database_managers.QDrantDBManager(\n",
    "    url=os.getenv('QDRANT_URL'),\n",
    "    port=6333,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vector_size=768,\n",
    "    embedding=embedding,\n",
    "    record_manager_url=r\"sqlite:///record_manager_cache.sql\"\n",
    ")\n",
    "vectore_store_client=vectore_store.vector_store\n",
    "retriever = vectore_store_client.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through the pdf in the folder \n",
    "import os\n",
    "\n",
    "folder_path=\"./documents\"\n",
    "docs=[]\n",
    "\n",
    "for pdf_file in os.listdir(folder_path):\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "\n",
    "        loader = PyPDFLoader(os.path.join(folder_path, pdf_file))\n",
    "        docs=docs+loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=text_splitter.fixed_split(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectore_store.index_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create custom prompt for your use case\n",
    "prompt_template=\"\"\"Sei Debbie, un'assistente alla ricerca AI che parla ai ricercatori universitari. \n",
    "\n",
    "Rispondi alle domande utilizzando i fatti forniti. \n",
    "Usa sempre cortesie e saluti gentili come \"Buongiorno\" e \"Buon pomeriggio\". \n",
    "Utilizza i seguenti pezzi di contesto per rispondere alla domanda degli utenti.\n",
    "Prendi nota delle fonti e includile nella risposta nel formato: \"SOURCES: source1 source2\", usa \"SOURCES\" in maiuscolo indipendentemente dal numero di fonti.\n",
    "\n",
    "Traduci sempre tutto in italiano.\n",
    "\n",
    "Se non conosci la risposta, dì semplicemente \"Non lo so\", non cercare di inventare una risposta.\n",
    "----------------\n",
    "{summaries}\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(prompt_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(temperature=0, model_name=\"Llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#build your chain for RAG+C\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "  ### Answer: \n",
       "  Buongiorno! L'adiabatic quantum computing, o AQC, è un approccio alternativo al modello standard di calcolo quantistico, che si basa sull'idea di evolvere gradualmente un sistema quantistico in modo da raggiungere lo stato finale desiderato. Questo approccio è stato proposto per la prima volta nel 2000 da Seth Lloyd e da altri ricercatori.\n",
       "\n",
       "In AQC, il sistema quantistico è descritto da un'hamiltoniana, che è un operatore matematico che descrive l'energia del sistema. L'obiettivo è quello di evolvere l'hamiltoniana in modo da raggiungere lo stato finale desiderato, che è il minimo di energia del sistema.\n",
       "\n",
       "L'adiabatic quantum computing è stato dimostrato essere un approccio universale per la computazione quantistica, cioè può eseguire qualsiasi calcolo quantistico che può essere eseguito con il modello standard di calcolo quantistico. Inoltre, AQC ha anche mostrato di avere un vantaggio quantistico rispetto ai calcoli classici, cioè può eseguire certi calcoli più velocemente di quanto non possa fare un computer classico.\n",
       "\n",
       "\n",
       "  ### Sources: \n",
       "  ./documents\\Adiabatic quantum computation.pdf\n",
       "  ### All relevant sources:\n",
       "  ./documents\\Adiabatic quantum computation.pdf\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"cos'è l'adiabatic quantum computing?\"\n",
    "result = chain(query)\n",
    "display(Markdown(print_result(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With documents download on chain with agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import Tool\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"input\", output_key=\"output\",return_messages=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sleep', 'wolfram-alpha', 'google-search', 'google-search-results-json', 'searx-search-results-json', 'bing-search', 'metaphor-search', 'ddg-search', 'google-lens', 'google-serper', 'google-scholar', 'google-finance', 'google-trends', 'google-jobs', 'google-serper-results-json', 'searchapi', 'searchapi-results-json', 'serpapi', 'dalle-image-generator', 'twilio', 'searx-search', 'merriam-webster', 'wikipedia', 'arxiv', 'golden-query', 'pubmed', 'human', 'awslambda', 'stackexchange', 'sceneXplain', 'graphql', 'openweathermap-api', 'dataforseo-api-search', 'dataforseo-api-search-json', 'eleven_labs_text2speech', 'google_cloud_texttospeech', 'read_file', 'reddit_search', 'news-api', 'tmdb-api', 'podcast-api', 'memorize', 'llm-math', 'open-meteo-api', 'requests', 'requests_get', 'requests_post', 'requests_patch', 'requests_put', 'requests_delete', 'terminal']\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.load_tools import get_all_tool_names\n",
    "print(get_all_tool_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duckduckgo-search\n",
      "  Downloading duckduckgo_search-6.1.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting click>=8.1.7 (from duckduckgo-search)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pyreqwest-impersonate>=0.4.6 (from duckduckgo-search)\n",
      "  Downloading pyreqwest_impersonate-0.4.6-cp38-abi3-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: orjson>=3.10.3 in c:\\users\\elafacrb1\\codice\\github\\langchain-rag-agent-chatbot\\.venv\\rag\\lib\\site-packages (from duckduckgo-search) (3.10.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\elafacrb1\\codice\\github\\langchain-rag-agent-chatbot\\.venv\\rag\\lib\\site-packages (from click>=8.1.7->duckduckgo-search) (0.4.6)\n",
      "Downloading duckduckgo_search-6.1.2-py3-none-any.whl (24 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading pyreqwest_impersonate-0.4.6-cp38-abi3-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.4/2.6 MB 12.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.2/2.6 MB 15.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.5/2.6 MB 19.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 16.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pyreqwest-impersonate, click, duckduckgo-search\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.3\n",
      "    Uninstalling click-8.1.3:\n",
      "      Successfully uninstalled click-8.1.3\n",
      "Successfully installed click-8.1.7 duckduckgo-search-6.1.2 pyreqwest-impersonate-0.4.6\n"
     ]
    }
   ],
   "source": [
    "!pip install -U duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events. You should ask targeted questions\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELAFACRB1\\Codice\\GitHub\\langchain-rag-agent-chatbot\\.venv\\rag\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "agent = initialize_agent(\n",
    "        agent=\"zero-shot-react-description\",\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        memory=memory,\n",
    "        return_intermediate_steps= True, # Make sure you set it to True\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "search: useful for when you need to answer questions about current events. You should ask targeted questions\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [search]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(agent.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@kbdhunga/exploring-langchain-chains-agents-a-quick-overview-9d0a8c4d7ba0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom tool \n",
    "\n",
    "def embed_papers(question : str) -> str:\n",
    "    pass\n",
    "    #TODO\n",
    "\n",
    "embed_papers_tool = Tool(\n",
    "    name=\"Embed papers\",\n",
    "    func=embed_papers,\n",
    "    description=\"Utile per cercare documenti relativi a paper non ancora ricercati durante le precedenti ricerche\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
